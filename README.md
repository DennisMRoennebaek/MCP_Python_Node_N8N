# MCP + FastAPI + n8n + Ollama Example

This project demonstrates how a traditional API can be integrated with
Large Language Models (LLMs) using the Model Context Protocol (MCP),
Node.js, and workflow automation.

It combines:

-   A Python FastAPI backend exposing simple tools
-   A Node.js Express MCP server that wraps the API as MCP tools
-   n8n for orchestration and workflow automation
-   A local Ollama LLM (Llama3.2:3b) for AI reasoning and tool calling

The goal is to show a minimal, real-world architecture for connecting
APIs to LLM agents.

------------------------------------------------------------------------

## Architecture Overview

User / n8n\
↓\
Ollama (Llama3.2:3b)\
↓\
Node Express MCP Server\
↓\
Python FastAPI Backend

------------------------------------------------------------------------

## Components

### FastAPI (Python)

-   Provides simple API endpoints (e.g. ping, add numbers)
-   Represents a typical backend service

### Node MCP Server (Express)

-   Wraps FastAPI endpoints as MCP tools
-   Handles:
    -   Tool registration
    -   MCP transport
    -   Request validation
    -   Tool execution

### n8n

-   Orchestrates automation flows
-   Connects the LLM to MCP tools
-   Enables workflows and integrations

### Ollama (Local LLM)

Runs the model: - llama3.2:3b

Handles: - reasoning - tool selection - argument generation

------------------------------------------------------------------------

## Purpose of This Project

This repository demonstrates:

-   How to expose an API as MCP tools
-   How LLMs can call backend services
-   How to integrate LLM reasoning into workflows
-   How to run everything locally
-   How to connect n8n with MCP-based tool systems

------------------------------------------------------------------------

## Tech Stack

-   Python (FastAPI)
-   Node.js + Express
-   MCP SDK
-   n8n
-   Docker / Docker Compose
-   Ollama
-   Llama3.2:3b model

------------------------------------------------------------------------

## Services

### FastAPI

Endpoints: - /ping - /add

### MCP Server (Node)

Tools: - ping_api - add_numbers

Handles: - MCP sessions - Tool validation - Tool execution - REST bridge
for n8n

### n8n

Used to: - trigger workflows - call MCP tools - connect LLM reasoning to
backend services

### Ollama

Runs the local LLM used for reasoning and tool selection.

------------------------------------------------------------------------

## Running the Project

### Requirements

-   Docker + Docker Compose
-   Ollama installed locally

Pull the model:

ollama pull llama3.2:3b

------------------------------------------------------------------------

### Start all services

docker compose up --build

Services:

-   FastAPI → http://localhost:8000
-   MCP Server → http://localhost:3000
-   n8n → http://localhost:5678
-   Ollama → http://localhost:11434

------------------------------------------------------------------------

## Example Flow

1.  User sends a request in n8n
2.  n8n calls the local Ollama model
3.  LLM decides to use a tool
4.  MCP server executes the tool
5.  MCP calls FastAPI
6.  Response flows back through MCP → n8n → user

------------------------------------------------------------------------

## What This Demonstrates

-   LLMs can safely call real APIs
-   MCP provides a structured tool layer
-   n8n enables orchestration and automation
-   Ollama enables fully local AI systems
-   Backend services can become AI-native

------------------------------------------------------------------------

## Future Extensions

Possible improvements:

-   authentication layer
-   more advanced tools
-   database integration
-   multi-agent workflows
-   streaming tool responses
-   production deployment setup

------------------------------------------------------------------------

## Key Idea

Move from:

"API-first backend"

to

"LLM-aware backend with tool interfaces"

Where:

-   APIs become tools
-   LLMs become orchestrators
-   Workflows become autonomous

------------------------------------------------------------------------

## License

MIT
